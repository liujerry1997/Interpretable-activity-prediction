# -*- coding: utf-8 -*-
"""11685 Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OMwKfoD9j-LHwh_-Qn87TrXYrMUC3W7c

#Check Hardware Info
"""

# gpu_info = !nvidia-smi
# gpu_info = '\n'.join(gpu_info)
# if gpu_info.find('failed') >= 0:
#   print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
#   print('and then re-execute this cell.')
# else:
#   print(gpu_info)

# from psutil import virtual_memory
# ram_gb = virtual_memory().total / 1e9
# print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

# if ram_gb < 20:
#   print('To enable a high-RAM runtime, select the Runtime > "Change runtime type"')
#   print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')
#   print('re-execute this cell.')
# else:
#   print('You are using a high-RAM runtime!')


import sys
sys.path.append("/usr/local/lib/python3.6/site-packages")

"""# Data Preprocessing"""

import sys
sys.path.append("/usr/local/lib/python3.6/site-packages")

import pandas as pd
import numpy as np

from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem

def convert_smiles_to_numpy(smiles, radius, num_bits):
    mol = Chem.MolFromSmiles(smiles)
    features_vec = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=num_bits)
    features = np.zeros((1,))
    DataStructs.ConvertToNumpyArray(features_vec, features)
    return features

training_set = pd.read_csv('interpretable-activity-prediction/data/training_set.csv')
training_set = training_set.loc[:,['SMILES', 'Activity']]
testing_set = pd.read_csv('interpretable-activity-prediction/data/test_set_filtered.csv')
testing_set = testing_set.loc[:,['SMILES', 'Activity']]

training_set

testing_set

radius = 2
num_bits = 2048

training_x = np.array([convert_smiles_to_numpy(smiles, radius, num_bits) for smiles in training_set['SMILES']])
x_test = np.array([convert_smiles_to_numpy(smiles, radius, num_bits) for smiles in testing_set['SMILES']])
training_y = ((training_set['Activity'] == 'Active') * 1.0).to_numpy()
y_test = ((testing_set['Activity'] == 'Active') * 1.0).to_numpy()

from sklearn.model_selection import train_test_split

x_train, x_val, y_train, y_val = train_test_split(training_x, training_y, test_size=0.2, random_state=23)

print('Active train:', np.sum(y_train == 1))
print('Inactive train:', np.sum(y_train == 0))
print('Active val:', np.sum(y_val == 1))
print('Inactive val:', np.sum(y_val == 0))
print('Active test:', np.sum(y_test == 1))
print('Inactive test:', np.sum(y_test == 0))

"""# Dataset and Data Loader"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import time

import matplotlib.pyplot as plt
# %matplotlib inline

cuda = torch.cuda.is_available()
device = torch.device('cuda' if cuda else 'cpu')
print(device)

class MyMLPDataset(Dataset):
    def __init__(self, X, Y):
        self.X = torch.from_numpy(X)
        self.Y = torch.from_numpy(Y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, index):
        X = self.X[index].float()
        Y = self.Y[index].long()
        return X, Y

class MyTDNNDataset(Dataset):
    def __init__(self, X, Y):
        self.X = torch.from_numpy(X)
        self.Y = torch.from_numpy(Y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, index):
        X = self.X[index].unsqueeze(0).float()
        Y = self.Y[index].long()
        return X, Y

def get_mlp_loader(train_data, train_labels, val_data, val_labels, test_data, test_labels, cuda, batch_size):
    if cuda:
        loader_args = dict(batch_size=batch_size, num_workers=16, pin_memory=True)
    else:
        loader_args = dict(batch_size=64)
    train_dataset = MyMLPDataset(train_data, train_labels)
    train_loader = DataLoader(train_dataset, shuffle=True, **loader_args)
    val_dataset = MyMLPDataset(val_data, val_labels)
    val_loader = DataLoader(val_dataset, shuffle=False, **loader_args)
    test_dataset = MyMLPDataset(test_data, test_labels)
    test_loader = DataLoader(test_dataset, shuffle=False, **loader_args)
    return train_loader, val_loader, test_loader

def get_tdnn_loader(train_data, train_labels, val_data, val_labels, test_data, test_labels, cuda, batch_size):
    if cuda:
        loader_args = dict(batch_size=batch_size, num_workers=16, pin_memory=True)
    else:
        loader_args = dict(batch_size=64)
    train_dataset = MyTDNNDataset(train_data, train_labels)
    train_loader = DataLoader(train_dataset, shuffle=True, **loader_args)
    val_dataset = MyTDNNDataset(val_data, val_labels)
    val_loader = DataLoader(val_dataset, shuffle=False, **loader_args)
    test_dataset = MyTDNNDataset(test_data, test_labels)
    test_loader = DataLoader(test_dataset, shuffle=False, **loader_args)
    return train_loader, val_loader, test_loader

"""# Training and Validation"""

def train(model, train_loader, optimizer, criterion, device, epoch, empty_cache=False):
    start = time.time()
    model.train()
    avg_loss = 0.0
    train_loss = []
    correct = 0
    total = 0

    for batch_num, (x, y) in enumerate(train_loader):
        x, y = x.to(device), y.to(device)
        
        optimizer.zero_grad()

        output = model(x)
        loss = criterion(output, y)
        
        loss.backward()
        optimizer.step()
        
        avg_loss += loss.item()
        train_loss.extend([loss.item()]*x.size()[0])

        predicted = torch.max(output.data, 1)[1]
        total += y.size(0)
        correct += (predicted == y).sum().item()

        if batch_num % 50 == 49:
            print('Epoch: {}\tBatch: {}\tAvg-Loss: {:.4f}\tElapsed Time: {:.4f}'.
                  format(epoch+1, batch_num+1, avg_loss/50, time.time()-start))
            avg_loss = 0.0    
        
        if empty_cache:
            torch.cuda.empty_cache()
            del x
            del y
            del loss
        
    end = time.time()
    print('Epoch: {}\tTraining Time: {:.4f}'.format(epoch+1, end-start))
    return np.mean(train_loss), correct / total

def valid(model, val_loader, criterion, device, epoch, empty_cache=False):
    start = time.time()
    model.eval()
    val_loss = []
    correct = 0
    total = 0

    with torch.no_grad():
        for batch_num, (x, y) in enumerate(val_loader):
            x, y = x.to(device), y.to(device)
            output = model(x)
            
            loss = criterion(output, y)
            val_loss.extend([loss.item()]*x.size()[0])

            predicted = torch.max(output.data, 1)[1]
            total += y.size(0)
            correct += (predicted == y).sum().item()

            if empty_cache:
                torch.cuda.empty_cache()
                del x
                del y
                del loss
    
    model.train()
    end = time.time()
    if epoch is None:
        print('Final Validation Time: {:.4f}'.format(end-start))
    else:
        print('Epoch: {}\tValidation Time: {:.4f}'.format(epoch+1, end-start))
    return np.mean(val_loss), correct / total

def test(model, test_loader, device, empty_cache):
    start = time.time()
    model.eval()
    predicted_test = []
    with torch.no_grad():
        for i, (x, y) in enumerate(test_loader):
            x = x.to(device)
            output = model(x)
            predicted = torch.max(output.data, 1)[1]
            predicted_test.extend(predicted.cpu().detach().numpy())
            
            if empty_cache:
                torch.cuda.empty_cache()
                del x
    
    end = time.time()
    print('Testing Time: {:.4f}'.format(end-start))
    return predicted_test

def train_model(model, model_name, train_loader, val_loader, optimizer, criterion, scheduler, device, start_epoch, num_epochs, train_losses, valid_losses, empty_cache=False):
    for epoch in range(start_epoch, num_epochs):
        print("Epoch: {}\tLearning Rate: {}".format(epoch+1, optimizer.param_groups[0]['lr']))
        
        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device, epoch, empty_cache)
        print('Epoch: {}\tTrain Loss: {:.5f}\tTrain Accuracy: {:.5f}'.format(epoch+1, train_loss, train_acc))
        train_losses.append(train_loss)
        
        val_loss, val_acc = valid(model, val_loader, criterion, device, epoch, empty_cache)
        print('Epoch: {}\tVal Loss: {:.5f}\tVal Accuracy: {:.5f}'.format(epoch+1, val_loss, val_acc))
        valid_losses.append(val_loss)
        
        scheduler.step(val_loss)
    
    return train_losses, valid_losses

def training_plot(a, b):
    plt.figure(1)
    plt.plot(a, 'b', label="train")
    plt.plot(b, 'g', label="valid")
    plt.title('Training/Valid Loss')
    plt.legend()
    plt.savefig('Loss Plot.png')
    plt.show()

"""# Linear Classifier"""

from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import roc_auc_score

linear = make_pipeline(StandardScaler(), SGDClassifier(max_iter=1000, tol=1e-3))
linear.fit(x_train, y_train)
train_predict = linear.predict(x_train)
train_acc = np.mean(train_predict == y_train)
print('Train Accuracy: {:.5f}'.format(train_acc))
val_predict = linear.predict(x_val)
val_acc = np.mean(val_predict == y_val)
print('Val Accuracy: {:.5f}'.format(val_acc))
test_predict = linear.predict(x_test)
test_acc = np.mean(test_predict == y_test)
print('Test Accuracy: {:.5f}'.format(test_acc))
auc = roc_auc_score(y_test, test_predict)
print('AUC Score: {:.5f}'.format(auc))

"""# SVM Classifier"""

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import roc_auc_score

svm = make_pipeline(StandardScaler(), SVC(gamma='auto'))
svm.fit(x_train, y_train)
train_predict = svm.predict(x_train)
train_acc = np.mean(train_predict == y_train)
print('Train Accuracy: {:.5f}'.format(train_acc))
val_predict = svm.predict(x_val)
val_acc = np.mean(val_predict == y_val)
print('Val Accuracy: {:.5f}'.format(val_acc))
test_predict = svm.predict(x_test)
test_acc = np.mean(test_predict == y_test)
print('Test Accuracy: {:.5f}'.format(test_acc))
auc = roc_auc_score(y_test, test_predict)
print('AUC Score: {:.5f}'.format(auc))

"""# MLP Model"""

mlp = nn.Sequential(
    nn.Linear(2048, 1024),
    nn.ReLU(inplace=True),
    nn.BatchNorm1d(1024),
    nn.Linear(1024, 512),
    nn.ReLU(inplace=True),
    nn.BatchNorm1d(512),
    nn.Linear(512, 256),
    nn.ReLU(inplace=True),
    nn.BatchNorm1d(256),
    nn.Linear(256, 128),
    nn.ReLU(inplace=True),
    nn.BatchNorm1d(128),
    nn.Linear(128, 2)
)
mlp.to(device)
print(mlp)

num_epochs = 100

model_name = 'Simple_MLP'

lr = 1e-3
weight_decay = 5e-6

start_epoch = 0
train_losses = []
valid_losses = []

empty_cache = True

batch_size = 64
train_loader, val_loader, test_loader = get_mlp_loader(x_train, y_train, x_val, y_val, x_test, y_test, cuda, batch_size)

optimizer = optim.Adam(mlp.parameters(), lr=lr, weight_decay=weight_decay)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=1, verbose=True)
criterion = nn.CrossEntropyLoss()

train_losses, valid_losses = train_model(mlp, model_name, train_loader, val_loader, optimizer, criterion, scheduler,
                                         device, start_epoch, num_epochs, train_losses, valid_losses, empty_cache)

training_plot(train_losses, valid_losses)

test_loss, test_acc = valid(mlp, test_loader, criterion, device, None, empty_cache)
print('Test Loss: {:.5f}\tTest Accuracy: {:.5f}'.format(test_loss, test_acc))

from sklearn.metrics import roc_auc_score
test_predict = test(mlp, test_loader, device, empty_cache)
auc = roc_auc_score(y_test, test_predict)
print('AUC Score: {:.5f}'.format(auc))

"""# TDNN With Padding Model"""

tdnn = nn.Sequential(
    nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=1),
    nn.ReLU(inplace=True),
    nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=1),
    nn.ReLU(inplace=True),
    nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=1),
    nn.ReLU(inplace=True),
    nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=2, padding=1),
    nn.ReLU(inplace=True),
    nn.Flatten(),
    nn.Linear(128, 2)
)
tdnn.to(device)
print(tdnn)

num_epochs = 100

model_name = 'Simple_TDNN'

lr = 1e-3
weight_decay = 5e-6

start_epoch = 0
train_losses = []
valid_losses = []

empty_cache = True

batch_size = 64
train_loader, val_loader, test_loader = get_tdnn_loader(x_train, y_train, x_val, y_val, x_test, y_test, cuda, batch_size)

optimizer = optim.Adam(tdnn.parameters(), lr=lr, weight_decay=weight_decay)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=1, verbose=True)
criterion = nn.CrossEntropyLoss()

train_losses, valid_losses = train_model(tdnn, model_name, train_loader, val_loader, optimizer, criterion, scheduler,
                                         device, start_epoch, num_epochs, train_losses, valid_losses, empty_cache)

training_plot(train_losses, valid_losses)

test_loss, test_acc = valid(tdnn, test_loader, criterion, device, None, empty_cache)
print('Test Loss: {:.5f}\tTest Accuracy: {:.5f}'.format(test_loss, test_acc))

from sklearn.metrics import roc_auc_score
test_predict = test(tdnn, test_loader, device, empty_cache)
auc = roc_auc_score(y_test, test_predict)
print('AUC Score: {:.5f}'.format(auc))

"""# TDNN Without Padding Model"""

tdnn = nn.Sequential(
    nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=2),
    nn.ReLU(inplace=True),
    nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=2),
    nn.ReLU(inplace=True),
    nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=2),
    nn.ReLU(inplace=True),
    nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=2),
    nn.ReLU(inplace=True),
    nn.Flatten(),
    nn.Linear(128, 2)
)
tdnn.to(device)
print(tdnn)

num_epochs = 100

model_name = 'Simple_TDNN'

lr = 0.15
weight_decay = 5e-5
momentum = 0.9

start_epoch = 0
train_losses = []
valid_losses = []

empty_cache = True

batch_size = 64
train_loader, val_loader, test_loader = get_tdnn_loader(x_train, y_train, x_val, y_val, x_test, y_test, cuda, batch_size)

optimizer = optim.SGD(tdnn.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=1, verbose=True)
criterion = nn.CrossEntropyLoss()

train_losses, valid_losses = train_model(tdnn, model_name, train_loader, val_loader, optimizer, criterion, scheduler,
                                         device, start_epoch, num_epochs, train_losses, valid_losses, empty_cache)

training_plot(train_losses, valid_losses)

test_loss, test_acc = valid(tdnn, test_loader, criterion, device, None, empty_cache)
print('Test Loss: {:.5f}\tTest Accuracy: {:.5f}'.format(test_loss, test_acc))

from sklearn.metrics import roc_auc_score
test_predict = test(tdnn, test_loader, device, empty_cache)
auc = roc_auc_score(y_test, test_predict)
print('AUC Score: {:.5f}'.format(auc))