{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "SGC_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB4nV_Kl2kkr",
        "outputId": "6ce0b6b5-da1e-4fc7-d227-e2fe44c6dfc6"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(\"V100 is three times faster than P100\")\n",
        "  print(gpu_info)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "V100 is three times faster than P100\n",
            "Sun Dec 13 17:00:13 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    25W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC46zB7mSu83",
        "outputId": "020be6ea-c647-4d65-80fc-10a84799d088"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/MyDrive/Deep Learning Project/Interpretable-activity-prediction\"\n",
        "# !ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Deep Learning Project/Interpretable-activity-prediction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtIPhq-zShEu"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "from statistics import *\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaXYGgSoShEv"
      },
      "source": [
        "## Generate normalized adjacency matrix and normalize the feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yid8s_8xShEv"
      },
      "source": [
        "def normalized_adjacency(adj):\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    row_sum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "def row_normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(x.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return x\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_yfgwXpShEv"
      },
      "source": [
        "## Preprocess input features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B_kxJMtShEv"
      },
      "source": [
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "\n",
        "def sgc_precompute(features, adj, degree):\n",
        "    adj = normalized_adjacency(adj)\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj).float()\n",
        "    features = torch.tensor(features)\n",
        "    for i in range(degree):\n",
        "        features = torch.spmm(adj, features)\n",
        "    return features   # aton_num * 133\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcK5nHrbGe1u"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq-vtWptShEv"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, X, A, Y, degree):\n",
        "        self.X = X\n",
        "        self.A = A\n",
        "        self.Y = Y\n",
        "        self.degree = degree\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "    def __getitem__(self, index):\n",
        "        X = sgc_precompute(self.X[index], self.A[index], self.degree)\n",
        "        return torch.sum(X, dim = 0, keepdim = False), torch.tensor(self.Y[index])\n",
        "\n",
        "\n",
        "class MyDataset_mol(Dataset):   # molecular feature added\n",
        "    def __init__(self, X, A, Y, degree, mol_feat):\n",
        "        self.X = X\n",
        "        self.A = A\n",
        "        self.Y = Y\n",
        "        self.degree = degree\n",
        "        self.mol_feat = mol_feat\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "    def __getitem__(self, index):\n",
        "        X = sgc_precompute(self.X[index], self.A[index], self.degree)\n",
        "        # sum_atom_feat = torch.sum(X, dim = 0)\n",
        "        sum_atom_feat = torch.mean(X, dim = 0)\n",
        "        all_feat = np.concatenate( (self.mol_feat[index] , sum_atom_feat), axis=0)\n",
        "        # print(\"sum_atom_feat\", sum_atom_feat)\n",
        "        # print(\"self.mol_feat\", np.array(self.mol_feat[index]))\n",
        "        # print(\"all_feat\", all_feat)\n",
        "        # return torch.tensor(all_feat).float(), torch.tensor(self.Y[index])\n",
        "        return sum_atom_feat, torch.tensor(self.Y[index])\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyOyAMmIuEzS"
      },
      "source": [
        "# Essemble training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RApufZlaShEv"
      },
      "source": [
        "def train(models, train_dataloaders, val_dataloaders, optimizers, schedulers, criterion, num_epoch, k, ensemble):\n",
        "    train_losses_list, val_losses_list, train_accs_list, val_accs_list, auc_scores_list = [], [], [], [], []\n",
        "    for i in range(ensemble):\n",
        "        print(\"training ensemble model\", i+1)\n",
        "\n",
        "        model = models[i]\n",
        "        train_dataloader = train_dataloaders[i]\n",
        "        val_dataloader = val_dataloaders[i]\n",
        "        optimizer = optimizers[i]\n",
        "        scheduler = schedulers[i]\n",
        "\n",
        "        true_labels = np.array([])\n",
        "        probs = np.array([])\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accs = []\n",
        "        val_accs = []\n",
        "        for epoch in range(num_epoch):\n",
        "            model.train()\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            running_loss = 0\n",
        "            for batch_idx, (X, Y) in enumerate(train_dataloader):\n",
        "                optimizer.zero_grad()\n",
        "                output = model.forward(X)\n",
        "                probs = np.append(probs, output[:,-1].cpu().detach().numpy())\n",
        "                true_labels = np.append(true_labels, Y.cpu().detach().numpy())\n",
        "                \n",
        "                pred = torch.argmax(output, dim = 1)\n",
        "\n",
        "                correct += (pred==Y).sum().item()\n",
        "                total += Y.size(0)\n",
        "                train_acc = correct/total\n",
        "                loss = criterion(output, Y) \n",
        "                running_loss += loss.item() * Y.size(0)\n",
        "                train_loss = running_loss/total\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "            train_accs.append(train_acc)\n",
        "            train_losses.append(train_loss)\n",
        "            val_acc, val_loss, _ = validate(model, val_dataloader, criterion, k)\n",
        "            scheduler.step(val_loss)\n",
        "            val_accs.append(val_acc)\n",
        "            val_losses.append(val_loss)\n",
        "            auc_scores = roc_auc_score(true_labels , probs) \n",
        "\n",
        "        train_losses_list.append(train_losses)\n",
        "        val_losses_list.append(val_losses)\n",
        "        train_accs_list.append(train_accs)\n",
        "        val_accs_list.append(val_accs)\n",
        "        auc_scores_list.append(auc_scores)\n",
        "\n",
        "        \n",
        "\n",
        "    # print(train_losses_list, val_losses_list)\n",
        "    if ensemble == 1:\n",
        "        return train_losses_list[0], val_losses_list[0], train_accs_list[0], val_accs_list[0], auc_scores_list[0]\n",
        "\n",
        "    else:\n",
        "        train_ave_loss = np.average( np.array(train_losses_list), axis = 0) \n",
        "        val_ave_loss = np.average( np.array(val_losses_list) , axis = 0 )\n",
        "        train_ave_acc = np.average( np.array(train_accs_list) , axis = 0)\n",
        "        val_ave_acc = np.average( np.array(val_accs_list) , axis = 0)\n",
        "\n",
        "        return   train_ave_loss, val_ave_loss, train_ave_acc, val_ave_acc, auc_scores_list\n",
        "\n",
        "def validate(model, dataloader, criterion, k):\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    true_labels = np.array([])\n",
        "    probs = np.array([])\n",
        "\n",
        "    preds = []\n",
        "    for batch_idx, (X, Y) in enumerate(dataloader):\n",
        "        \n",
        "        output = model.forward(X)\n",
        "        pred = torch.argmax(output, dim = 1)\n",
        "        # preds.append(pred)\n",
        "        num_correct += (pred==Y).sum().item()\n",
        "        total += Y.size(0)\n",
        "        loss = criterion(output, Y)\n",
        "        running_loss += loss.item() * Y.size(0)\n",
        "        # print(\"preds\", preds)\n",
        "        probs = np.append(probs, output[:,-1].cpu().detach().numpy())\n",
        "        true_labels = np.append(true_labels, Y.cpu().detach().numpy())\n",
        "\n",
        "    auc_scores = roc_auc_score(true_labels , probs) \n",
        "\n",
        "    return num_correct/total, running_loss/total, auc_scores\n",
        "\n",
        "\n",
        "def validate_conf_matrix(model, dataloader, criterion, k):\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    true_labels = np.array([])\n",
        "    probs = np.array([])\n",
        "\n",
        "    preds = np.array([])\n",
        "    for batch_idx, (X, Y) in enumerate(dataloader):\n",
        "        \n",
        "        output = model.forward(X)\n",
        "        pred = torch.argmax(output, dim = 1)\n",
        "        preds = np.append(preds, pred.cpu().detach().numpy())\n",
        "        num_correct += (pred==Y).sum().item()\n",
        "        total += Y.size(0)\n",
        "        loss = criterion(output, Y)\n",
        "        running_loss += loss.item() * Y.size(0)\n",
        "        # print(\"preds\", preds)\n",
        "        probs = np.append(probs, output[:,-1].cpu().detach().numpy())\n",
        "        true_labels = np.append(true_labels, Y.cpu().detach().numpy())\n",
        "\n",
        "    auc_scores = roc_auc_score(true_labels , probs) \n",
        "\n",
        "    return num_correct/total, running_loss/total, auc_scores, true_labels, preds\n"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q-PMKuHq8kx"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtCXXEpHShEv"
      },
      "source": [
        "class SGC(nn.Module):\n",
        "    \"\"\"\n",
        "    A Simple PyTorch Implementation of Logistic Regression.\n",
        "    Assuming the features have been preprocessed with k-step graph propagation.\n",
        "    \"\"\"\n",
        "    def __init__(self, nfeat, nclass):\n",
        "        super(SGC, self).__init__()\n",
        "\n",
        "        self.W = nn.Linear(nfeat, nclass)\n",
        "        self.sm = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.W(x)\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.normal_(m.weight)\n",
        "        # m.bias.data.fill_(0.01)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KasHv6Z3ShEw"
      },
      "source": [
        "##############\n",
        "ensemble = 20\n",
        "##############\n",
        "\n",
        "# n_feature = 133 + 200\n",
        "n_feature = 133\n",
        "n_class = 2\n",
        "k = 6\n",
        "num_epoch = 30\n",
        "lr = 0.001\n",
        "wd = 5e-6\n",
        "\n",
        "SGC_base_list = []\n",
        "optimizer_list = []\n",
        "scheduler_list = []\n",
        "\n",
        "for i in range(ensemble):\n",
        "    SGC_base = SGC(n_feature, n_class)\n",
        "    SGC_base.apply(init_weights)\n",
        "    SGC_base_list.append(SGC_base)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(SGC_base.parameters(), lr = lr, weight_decay = wd)\n",
        "    optimizer_list.append(optimizer)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience = 0)\n",
        "    scheduler_list.append(scheduler)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3ViEwKGShEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eaea03b-2d42-4a3e-b0ce-5b63afdb59f3"
      },
      "source": [
        "# Split data and labels into postive and negative\n",
        "def split_pos_neg(X_set, Y):\n",
        "    pos_X_set = []\n",
        "    neg_X_set = []\n",
        "    for i in range(len(X_set)):\n",
        "        if Y[i] == 1:\n",
        "            X_set_Y = np.append(X_set[i], [1])\n",
        "            # print(X_set_Y.shape)\n",
        "            pos_X_set.append(X_set_Y)\n",
        "        else:\n",
        "            X_set_Y = np.append(X_set[i], [0])\n",
        "            neg_X_set.append(X_set_Y)\n",
        "    return np.array(pos_X_set), np.array(neg_X_set)\n",
        "\n",
        "train_f = np.load(\"data/train_feature.npy\", allow_pickle = True)\n",
        "train_adj_mx = np.load(\"data/train_adjacent_matrix.npy\", allow_pickle = True)\n",
        "train_mol_f = np.load(\"data/train_mol_features.npy\", allow_pickle = True)\n",
        "train_label = np.load(\"data/train_label.npy\", allow_pickle = True)\n",
        "\n",
        "# print(\"train_f\", train_f.shape)\n",
        "# print(\"train_adj_mx\", train_adj_mx.shape)\n",
        "# print(\"train_mol\", train_mol_f.shape)\n",
        "\n",
        "# concatenate atom features and adjancency matrix\n",
        "train_dataloader_list = []\n",
        "val_dataloader_list = []\n",
        "\n",
        "\n",
        "concat_f = []\n",
        "for count in range(train_f.shape[0]):   #2335\n",
        "    one_mol_f = []\n",
        "    one_mol_f.extend( [train_f[count], train_adj_mx[count], train_mol_f[count] ] )\n",
        "    concat_f.append(one_mol_f)\n",
        "\n",
        "concat_f = np.asarray(concat_f)\n",
        "train_X_set, val_X_set, train_Y, val_Y = model_selection.train_test_split(concat_f, train_label, test_size = 0.2, random_state = 42)\n",
        "\n",
        "pos_X_set, neg_X_set = split_pos_neg(train_X_set, train_Y)  # Take all \n",
        "\n",
        "print(pos_X_set.shape, neg_X_set.shape)\n",
        "\n",
        "for i in range(ensemble):  \n",
        "    # For each ensemble, pick all positive samples and same number of negative samples. (balanced 1:1)\n",
        "    mol_number = neg_X_set.shape[0]\n",
        "    random_indices = np.random.choice(mol_number, size=len(pos_X_set)*2 , replace=False)\n",
        "    part_neg_X_set = neg_X_set[random_indices]\n",
        "\n",
        "    train_X_set =  np.concatenate((pos_X_set, part_neg_X_set))\n",
        "    # print(train_X_set[:, 3])\n",
        "\n",
        "    np.random.shuffle(train_X_set)\n",
        "    # print(train_X_set[:, 3])\n",
        "\n",
        "    train_X = train_X_set[:,0]\n",
        "    train_A = train_X_set[:,1]\n",
        "    train_mol_X = train_X_set[:,2]\n",
        "    train_Y = train_X_set[:,3]   # This partial train_Y overwrites the overall train_Y before.\n",
        "\n",
        "    val_X = val_X_set[:,0]\n",
        "    val_A = val_X_set[:,1]\n",
        "    val_mol_f = val_X_set[:,2]\n",
        "\n",
        "    degree = 8\n",
        "\n",
        "    train_dataset = MyDataset_mol(train_X, train_A, train_Y, degree, train_mol_X)\n",
        "    val_dataset = MyDataset_mol(val_X, val_A, val_Y, degree, val_mol_f)\n",
        "\n",
        "    train_dataloader_args = dict(shuffle = True, batch_size = 8, drop_last = True)\n",
        "    val_dataloader_args = dict(shuffle = False, batch_size = 8, drop_last = True)\n",
        "    train_dataloader = DataLoader(train_dataset, **train_dataloader_args)\n",
        "    val_dataloader = DataLoader(val_dataset, **val_dataloader_args)\n",
        "\n",
        "    train_dataloader_list.append(train_dataloader)\n",
        "    val_dataloader_list.append(val_dataloader)\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 4) (1768, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtfFJdosU8ZL"
      },
      "source": [
        "# Run Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ZpsefFShEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c6bad7-d239-40bd-d4b0-60b6e0ff5342"
      },
      "source": [
        "train_losses, val_losses, train_accs, val_accs, auc_scores = train(SGC_base_list, train_dataloader_list, val_dataloader_list, \n",
        "                                                       optimizer_list, scheduler_list, criterion, num_epoch, k, ensemble)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training ensemble model 1\n",
            "training ensemble model 2\n",
            "training ensemble model 3\n",
            "training ensemble model 4\n",
            "training ensemble model 5\n",
            "training ensemble model 6\n",
            "training ensemble model 7\n",
            "training ensemble model 8\n",
            "training ensemble model 9\n",
            "training ensemble model 10\n",
            "training ensemble model 11\n",
            "training ensemble model 12\n",
            "training ensemble model 13\n",
            "training ensemble model 14\n",
            "training ensemble model 15\n",
            "training ensemble model 16\n",
            "training ensemble model 17\n",
            "training ensemble model 18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocL37X58iWxE"
      },
      "source": [
        "model = SGC_base_list[0]\n",
        "# for param in model.parameters():\n",
        "#   print(param.data.shape)\n",
        "#   print(param.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsRalJXMShEw"
      },
      "source": [
        "plt.plot(train_losses, label = \"train_loss\")\n",
        "plt.plot(val_losses, label = \"val_loss\")\n",
        "# plt.plot(train_accs, label = \"train_acc\")\n",
        "# plt.plot(val_accs, label = \"val_acc\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"training epoch\")\n",
        "\n",
        "# plt.savefig(\"/content/drive/MyDrive/Deep Learning Project/output_fig/ensemble_mol_loss.jpg\")\n",
        "# plt.savefig(\"/content/drive/MyDrive/Deep Learning Project/output_fig/ensemble_mol_acc.jpg\")\n",
        "\n",
        "# print(\"train_accs\", train_accs)\n",
        "# print(\"val_accs\", val_accs)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW9URhm6ShEw"
      },
      "source": [
        "test_X = np.load(\"data/test_feature.npy\", allow_pickle = True)\n",
        "test_A = np.load(\"data/test_adjacent_matrix.npy\", allow_pickle = True)\n",
        "test_mol_f = np.load(\"data/test_mol_features.npy\", allow_pickle = True)\n",
        "test_Y = np.load(\"data/test_label.npy\", allow_pickle = True)\n",
        "\n",
        "\n",
        "test_dataset = MyDataset_mol(test_X, test_A, test_Y, k, test_mol_f)\n",
        "test_dataloader = DataLoader(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H1TXIyqShEw"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "test_acc_list = []\n",
        "auc_list = []\n",
        "# print(SGC_base_list)\n",
        "\n",
        "\n",
        "for i in range(ensemble):\n",
        "    SGC_base = SGC_base_list[i]\n",
        "    test_acc, _, auc_score, true_labels, preds  = validate_conf_matrix(SGC_base, test_dataloader, criterion, k)\n",
        "    test_acc_list.append(test_acc)\n",
        "    auc_list.append(auc_score)\n",
        "\n",
        "print(\"label\",list(test_Y))\n",
        "\n",
        "cm = confusion_matrix(true_labels, preds)\n",
        "print(cm)\n",
        "# plt.imshow(cm, cmap='binary')\n",
        "\n",
        "matrix_df = pd.DataFrame(cm, index=['True_Inactive', 'True_Active'], columns=['Pred_Inactive', 'Pred_Active'])\n",
        "ax=sns.heatmap(matrix_df, cmap='YlGnBu', annot=True, fmt='d', annot_kws={'size': 16}).set_title('SGC ensemble Confusion Matrix')\n",
        "plt.savefig('/content/drive/MyDrive/Deep Learning Project/output_fig/{}_matrix.png'.format(\"SGC_ensmeble\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbM14JGaShEw"
      },
      "source": [
        "print(test_acc_list)\n",
        "print(sum(test_acc_list)/ensemble)\n",
        "print(auc_list)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}